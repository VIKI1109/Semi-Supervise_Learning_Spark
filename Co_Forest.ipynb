{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Random-Forest Implentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 15:04:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "conf =SparkConf().setMaster(\"local[*]\").setAppName(\"CW\") \n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# change the path here to the path of your dataset\n",
    "path = \"./new_feature.csv\"\n",
    "df = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import allclose\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from  pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=df.drop('Class').columns,outputCol=\"features\")\n",
    "df=assembler.transform(df)\n",
    "\n",
    "indexer = StringIndexer(inputCol = 'Class', outputCol = 'ClassIndex')\n",
    "df = indexer.fit(df).transform(df)\n",
    "df=df.select('features', 'ClassIndex')\n",
    "\n",
    "# change the parameters here ti set the labeled and unlabeled ratio and training/test ratio\n",
    "unlabel_ratio=0.95\n",
    "test_ratio=0.10\n",
    "num_of_Base_Classifier=6\n",
    "rand_seed=np.random.randint(99999)\n",
    "\n",
    "train, test = df.randomSplit([1-test_ratio, test_ratio], seed=rand_seed)\n",
    "L_train, U_train = train.randomSplit([1-unlabel_ratio, unlabel_ratio], seed=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation of Co-Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import concat, col, lit,struct,udf, array\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Row\n",
    "import functools\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "class CoForest_Config:\n",
    "    def __init__(self,num_classifiers=6,m_numFeatures=0,m_seed=1,m_KValue=0,m_threshold=0.75,max_iter=10000,m_classifiers=None):\n",
    "        self.num_classifiers=num_classifiers\n",
    "        self.m_numFeatures=m_numFeatures\n",
    "        self.m_seed=m_seed\n",
    "        self.m_KValue=m_KValue\n",
    "        self.m_threshold=m_threshold\n",
    "        self.m_numOriginalLabeledInsts=0\n",
    "        self.m_classifiers=m_classifiers\n",
    "        self.max_iter=max_iter\n",
    "        self.num_class=0\n",
    "# you can change the configuration of training there\n",
    "coforest_conf=CoForest_Config() # the name must be exactly coforest_conf\n",
    "num_class=5\n",
    "num_classifiers=coforest_conf.num_classifiers\n",
    "m_threshold=coforest_conf.m_threshold\n",
    "        \n",
    "def getPredictionOnLabelData(Instances,m_classifiers):\n",
    "    index_Ins = Instances.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    L_data = Instances.map(lambda x:x[0]).toDF()\n",
    "    predic=m_classifiers[0].evaluate(L_data)\n",
    "    predic=predic.predictions.select(\"prediction\",\"probability\")\n",
    "    columns = [col(\"prediction\"),col(\"probability\")] \n",
    "    predicRDD=predic.withColumn(\"outcome\", struct(columns)).select(\"outcome\").rdd.zipWithIndex().map(lambda x: (x[1],(x[0],)))\n",
    "    for i in range(1,len(m_classifiers)):\n",
    "        predic.unpersist()\n",
    "        predic=m_classifiers[i].evaluate(L_data).predictions.select(\"prediction\",\"probability\")\n",
    "        tempRDD=predic.withColumn(\"outcome\", struct(columns)).select(\"outcome\").rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        predicRDD=predicRDD.join(tempRDD).map(lambda x:(x[0],(*x[1][0],x[1][1])))\n",
    "    outCome=predicRDD.join(index_Ins)\n",
    "    outCome=outCome.sortBy(lambda x: x[0]).map(lambda x:(x[1][0],x[1][1][1],x[1][1][2],x[1][1][0]))\n",
    "    # release Memory\n",
    "    outCome.take(1)\n",
    "    index_Ins.unpersist()\n",
    "    predic.unpersist()\n",
    "    L_data.unpersist()\n",
    "    predicRDD.unpersist()\n",
    "    return outCome\n",
    "\n",
    "def blockSample(iter1):\n",
    "    numblock=10000      # 块的大小\n",
    "    randomindexblock={} # 块内每行的随机抽样的行\n",
    "    i= -1\n",
    "    for x in iter1:\n",
    "        i+=1\n",
    "        index=i%numblock\n",
    "        if(index==0):\n",
    "            randomindexblock = gen_randomindexblock(numblock)\n",
    "        if(index in randomindexblock.keys()):\n",
    "            yield (x[0],x[1] + [randomindexblock[index]]) # 样本x被随机选中了,并且选中了randomindexblock(index)次,插在后面\n",
    "        else:\n",
    "            yield (x[0],x[1] + [0])\n",
    "\n",
    "def gen_randomindexblock(numblock):\n",
    "    result={}\n",
    "    tmp=-1\n",
    "    for j in range(numblock):\n",
    "            # 随机生成一个数据,属于[0,numblock)\n",
    "        tmp=random.randint(0,numblock)\n",
    "        if not tmp in result.keys():  # 如果不存在则新建\n",
    "            result[tmp] = 0\n",
    "        result[tmp] = result[tmp]+1\n",
    "    return result\n",
    "\n",
    "def blockSample2(classify_i,iter1):    \n",
    "    for x in iter1:\n",
    "        samplenum=x[1][classify_i] ## 抽样次数\n",
    "        if samplenum>0: # 如果抽样次数大于0\n",
    "            for y in range(0,samplenum): # 遍历抽样的次数,每次重复输出一次\n",
    "                yield (x[0],1.0)       # weight=1.0\n",
    "\n",
    "def resampleWithWeightsforallclassify(Config,Dataset):\n",
    "    labeled_tmp=None \n",
    "    index=0 \n",
    "    labeled_tmp=Dataset.map(lambda x: (x, []))\n",
    "    for i in range(0,Config.num_classifiers):\n",
    "        labeled_tmp=labeled_tmp.mapPartitions(blockSample)\n",
    "\n",
    "    labeleds_outofbag=labeled_tmp.map(lambda x: (x[0],1.0, [i > 0 for i in x[1]]))\n",
    "    # 每个分类器的抽样的袋内数据:根据抽样次数,某些样本会出现多次,有些样本会不出现\n",
    "    #　Array[RDD[InstLabeledinofbag]]\n",
    "    labeleds_inofbag=[]\n",
    "    # 遍历分类器\n",
    "    for i in range(0,Config.num_classifiers):\n",
    "        tmp_func=functools.partial(blockSample2,i)\n",
    "        labeleds_inofbag.append(labeled_tmp.mapPartitions(tmp_func))\n",
    "    return (labeleds_inofbag,labeleds_outofbag)\n",
    "\n",
    "def reduceError(input1,input2):\n",
    "    return (input1[0]+input2[0],input1[1]+input2[1])\n",
    "\n",
    "def measureError(m_threshold,Instances,id1):\n",
    "    mapFunc=functools.partial(errorSample,m_threshold,id1)\n",
    "    (err,count)=Instances.mapPartitions(mapFunc).reduce(reduceError)\n",
    "    return err/count\n",
    "\n",
    "def errorSample(m_threshold,id1,iter1):\n",
    "    err=0\n",
    "    count=0\n",
    "    for x in iter1:\n",
    "        distr=outOfBagDistributionForInstanceExcluded((x[0],x[2]),id1)\n",
    "        if(getConfidence(distr)>m_threshold): # add self.m_class\n",
    "            count += x[1]\n",
    "            if int(maxIndex(distr))!=int(x[3][\"ClassIndex\"]):\n",
    "                err += x[1]\n",
    "    yield (err,count)\n",
    "    \n",
    "# override def distributionForInstance(Instance:Vector): Array[Double] ={\n",
    "#     val index:Int=solModel_lr.predict(Instance).toInt\n",
    "#     val result:Array[Double]=new Array(10)\n",
    "#     result(index)=1.0\n",
    "#     result\n",
    "#   } 就是返回一个one-hot vector, 分类的class标记为1,所以此处不使用distribution,直接返回prediction class index\n",
    "#  当然 也可以是用 c[0].evaluate(test).predictions.take(20)[0][\"probability\"] 来获得distribution\n",
    "def outOfBagDistributionForInstanceExcluded(Instance,idExcluded):\n",
    "    distr = [0 for i in range(0,num_class)] # replace with number of classes\n",
    "#     print(Instance,\" \",idExcluded)\n",
    "    for i in range(0,num_classifiers): # replace with num_classifier\n",
    "        if(Instance[1][i]==False and i!=idExcluded):\n",
    "#             d=m_classifiers[i].distributionForInstance(Instance[0]) # add self.m_class\n",
    "#             ins=sc.parallelize([Instance[0]]).toDF()\n",
    "#             d=m_classifiers[i].evaluate(ins).predictions.head()[0][\"prediction\"]\n",
    "#             ins.unpersist()\n",
    "#             print(Instance[0])\n",
    "            d=Instance[0][i][\"outcome\"][\"prediction\"]\n",
    "#             print(\"prediction is \",d)\n",
    "#         for iClass in range(0,CF_cofig.Get_numClasses):\n",
    "            distr[int(d)]+=1\n",
    "    if(sum(distr)!= 0):\n",
    "        distr=normalize(distr) # add self.m_class\n",
    "    return distr\n",
    "\n",
    "def normalize(inputs):\n",
    "    sumVal=0\n",
    "    output=[0 for i in range(len(inputs))]\n",
    "    for i in range(len(inputs)):\n",
    "        sumVal+=inputs[i]\n",
    "    for i in range(len(inputs)):\n",
    "        output[i]=inputs[i]/sumVal\n",
    "    return output\n",
    "\n",
    "def getConfidence(p):\n",
    "    return p[maxIndex(p)]\n",
    "\n",
    "def maxIndex(inputs):\n",
    "    a = np.argmax(np.array(inputs))\n",
    "    return np.argmax(np.array(inputs))\n",
    "\n",
    "\n",
    "def sampleWithWeightsforoneclassify(Dataset,Datasetnum,samplenum):\n",
    "    return Dataset.sample(withReplacement=False, \\\n",
    "                   fraction=min(1.0,float(samplenum)/float(Datasetnum)), \\\n",
    "                   seed=current_milli_time())\n",
    "    \n",
    "def getPredictionOnUnLabelData(Instances,m_classifiers):\n",
    "    index_Ins = Instances.rdd.zipWithIndex().map(lambda x: (x[1],(1.0,x[0])))\n",
    "    predic=m_classifiers[0].evaluate(Instances)\n",
    "    predic=predic.predictions.select(\"prediction\",\"probability\")\n",
    "    columns = [col(\"prediction\"),col(\"probability\")] \n",
    "    predicRDD=predic.withColumn(\"outcome\", struct(columns)).select(\"outcome\").rdd.zipWithIndex().map(lambda x: (x[1],(x[0],)))\n",
    "    for i in range(1,len(m_classifiers)):\n",
    "        predic=m_classifiers[i].evaluate(Instances).predictions.select(\"prediction\",\"probability\")\n",
    "        tempRDD=predic.withColumn(\"outcome\", struct(columns)).select(\"outcome\").rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        predicRDD=predicRDD.join(tempRDD).map(lambda x:(x[0],(*x[1][0],x[1][1])))\n",
    "    outCome=predicRDD.join(index_Ins)\n",
    "    outCome=outCome.sortBy(lambda x: x[0]).map(lambda x:(x[1][0],x[1][1][0],x[1][1][1]))\n",
    "    \n",
    "    # release Memory\n",
    "    outCome.take(1)\n",
    "    index_Ins.unpersist()\n",
    "    predic.unpersist()\n",
    "    tempRDD.unpersist()\n",
    "    predicRDD.unpersist()\n",
    "    return outCome\n",
    "\n",
    "\n",
    "def distributionForInstanceExcluded(Instance,idExcluded):\n",
    "    res=[0.0 for i in range(0,num_class)]\n",
    "    for i in range(0,num_classifiers):\n",
    "        if(i!=idExcluded):\n",
    "            d=Instance[0][i][\"outcome\"][\"prediction\"]\n",
    "            res[int(d)]+=1\n",
    "    return normalize(res)\n",
    "    \n",
    "def isHighConfidence(Instance,idExcluded):\n",
    "    distr = distributionForInstanceExcluded(Instance,idExcluded) # 使用除第idExcluded以外的所有分类器对未标记样本inst综合预测出该样本属于每个分类的概率\n",
    "    confidence = getConfidence(distr) # 找出每个分类概率中最max的概率作为置信度\n",
    "    if(confidence > m_threshold): # 如果置信度大于阀值\n",
    "        predictlabel = maxIndex(distr) # 找出每个分类概率中第几个分类是最max的,作为分类结果\n",
    "        return (True,predictlabel,confidence) # 能成为标记数据集\n",
    "    else:\n",
    "        return (False,-1,0.0) # 不能成为标记数据集\n",
    "\n",
    "def sampleUnlabel(classify_i,iter1):\n",
    "    for x in iter1:\n",
    "        (isConfident,predictlabeled,weight)=isHighConfidence(x,classify_i)\n",
    "        if isConfident:\n",
    "            yield (x[2],predictlabeled,weight)\n",
    "            \n",
    "def modelClassify(Instances,m_classifiers):\n",
    "    index_Ins = Instances.rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    predic=m_classifiers[0].evaluate(Instances)\n",
    "    predicRDD=predic.predictions.select(\"prediction\").rdd.zipWithIndex().map(lambda x: (x[1],(x[0],)))\n",
    "    for i in range(1,len(m_classifiers)):\n",
    "        predic=m_classifiers[i].evaluate(Instances).predictions.select(\"prediction\")\n",
    "        tempRDD=predic.rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        predicRDD=predicRDD.join(tempRDD).map(lambda x:(x[0],(*x[1][0],x[1][1])))\n",
    "    predicRDD=predicRDD.mapPartitions(getLabelBasedOnVoting)\n",
    "    outCome=index_Ins.join(predicRDD).map(lambda x: Row(features=x[1][0][\"features\"],ClassIndex=x[1][0][\"ClassIndex\"], \\\n",
    "                                              prediction=x[1][1])).toDF()\n",
    "    # release Memory\n",
    "    outCome.take(1)\n",
    "    index_Ins.unpersist()\n",
    "    predic.unpersist()\n",
    "    tempRDD.unpersist()\n",
    "    predicRDD.unpersist()\n",
    "    return outCome\n",
    "\n",
    "def getLabelBasedOnVoting(iter1):\n",
    "    for x in iter1:\n",
    "        distr=distributionForInstance(x[1])\n",
    "        yield (x[0],float(maxIndex(distr)))\n",
    "            \n",
    "def distributionForInstance(Instance):\n",
    "    res=[0.0 for i in range(0,num_class)]\n",
    "    for i in range(0,num_classifiers):\n",
    "        d=Instance[i][\"prediction\"]\n",
    "        res[int(d)]+=1\n",
    "    return normalize(res)\n",
    "            \n",
    "\n",
    "def buildClassifier(Config,labeledSet,unlabeledSet):\n",
    "    err =  np.zeros(Config.num_classifiers)\n",
    "    err_prime = np.zeros(Config.num_classifiers)\n",
    "    s_prime = np.zeros(Config.num_classifiers)\n",
    "    s = np.zeros(Config.num_classifiers)\n",
    "    global num_classifiers\n",
    "    num_classifiers=Config.num_classifiers\n",
    "    \n",
    "    random.seed(Config.m_seed)\n",
    "    Config.m_numOriginalLabeledInsts = labeledSet.count()\n",
    "    m_KValue = Config.m_numFeatures\n",
    "    if (m_KValue < 1):\n",
    "        m_KValue = int(math.log2(len(labeledSet.columns)))+1\n",
    "        Config.m_numFeatures=m_KValue\n",
    "    \n",
    "    Config.num_class=df.groupBy(\"ClassIndex\").count().count()\n",
    "    global num_class\n",
    "    num_class=Config.num_class\n",
    "    \n",
    "    randSeeds = np.random.randint(99999, size=Config.num_classifiers)\n",
    "    m_classifiers = []\n",
    "    # you can change the configuation of base classifier here\n",
    "    classifier_config = RandomForestClassifier(numTrees=1,maxDepth=12,maxBins=32,labelCol='ClassIndex',featuresCol='features',seed=Config.m_seed,bootstrap=False)\n",
    "    \n",
    "    (labeleds_inofbag,labeleds_outofbag)=resampleWithWeightsforallclassify(Config,labeledSet.rdd)\n",
    "    for i in range(0,Config.num_classifiers):\n",
    "        classifier_config.setSeed(randSeeds[i])\n",
    "        m_classifiers.append(classifier_config.fit(labeleds_inofbag[i].map(lambda x:x[0]).toDF()))\n",
    "        err_prime[i] = 0.5;\n",
    "        s_prime[i] = 0;\n",
    "\n",
    "    Config.m_classifiers=m_classifiers\n",
    "    bChanged = True;\n",
    "    Iteration=0\n",
    "    Li=[0 for i in range(0,Config.num_classifiers)]\n",
    "\n",
    "    while(bChanged & (Iteration < Config.max_iter)):\n",
    "        bChanged = False;\n",
    "        bUpdate = [False for i in range(Config.num_classifiers)]\n",
    "        predic_data=getPredictionOnLabelData(labeleds_outofbag,m_classifiers)\n",
    "        for i in range(0,Config.num_classifiers):\n",
    "            err[i] = measureError(Config.m_threshold,predic_data,i)\n",
    "            Li[i] = 0\n",
    "            if(err[i] < err_prime[i]):\n",
    "                if(s_prime[i] == 0):\n",
    "                    s_prime[i] = min(unlabeledSet.count() / 10, 1000)\n",
    "                weight = 0             \n",
    "                numWeightsAfterSubsample = int(math.ceil(err_prime[i] * s_prime[i] / err[i] - 1))\n",
    "                \n",
    "                subSamplingSet=sampleWithWeightsforoneclassify(unlabeledSet,unlabeledSet.count(),numWeightsAfterSubsample)\n",
    "                tmp_func=functools.partial(sampleUnlabel,i)\n",
    "                un_predic_data=getPredictionOnUnLabelData(subSamplingSet,m_classifiers).mapPartitions(tmp_func)\n",
    "                s[i]=un_predic_data.map(lambda x: x[2]).reduce(lambda x,y:x+y)\n",
    "                Li[i]=un_predic_data.map(lambda x: Row(features=x[0][\"features\"],ClassIndex=float(x[1]))).toDF()\n",
    "                Li_i_size=Li[i].count()\n",
    "                if(s_prime[i] < float(Li_i_size)):\n",
    "                    if(err[i]*s[i] < err_prime[i]*s_prime[i]):\n",
    "                        bUpdate[i] = True\n",
    "                # release Memory\n",
    "                un_predic_data.unpersist()\n",
    "                subSamplingSet.unpersist()\n",
    "                \n",
    "        \n",
    "        # update classifiers\n",
    "        for i in range(Config.num_classifiers):\n",
    "            if(bUpdate[i]):\n",
    "                bChanged = True\n",
    "                newDataSet=labeledSet.union(Li[i])\n",
    "                m_classifiers[i]=classifier_config.fit(newDataSet)\n",
    "                newDataSet.unpersist()\n",
    "                err_prime[i] = err[i];\n",
    "                s_prime[i] = s[i];\n",
    "        Iteration+=1\n",
    "        \n",
    "        # release Memory\n",
    "        predic_data.unpersist()\n",
    "        for i in range(0,len(Li)):\n",
    "            if Li[i] == 0:\n",
    "                continue\n",
    "            Li[i].unpersist()\n",
    "        print(\"The iteration: \",Iteration)\n",
    "\n",
    "    return Iteration,m_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 15:04:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "/opt/bitnami/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration:  3\n"
     ]
    }
   ],
   "source": [
    "## to-do:目前看来，还有两个错误:\n",
    "# 1.为什么confidence一直是1啊，奇怪(解决了，normailize写错了,无语)\n",
    "# 2.把unlabeled data的ClassIndex换成之前训练器得出的结果，不要用原本的label去训练下一个训练器(解决了,toDF一直infer schema error,把ClassIndex换成float就没有错误了,不知道spark为什么这么设计,实在是不好用)\n",
    "# 3.内存使用的太厉害啦，用memory tuning的一些技巧和unpersist()来减少内存使用\n",
    "\n",
    "coforest_conf.max_iter=100\n",
    "iter_times,CORF_classifiers=buildClassifier(coforest_conf,L_train,U_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|            features|ClassIndex|prediction|\n",
      "+--------------------+----------+----------+\n",
      "|[-108.55273845353...|       3.0|       1.0|\n",
      "|[-11.633480773034...|       1.0|       1.0|\n",
      "|[-7.6028835753394...|       4.0|       4.0|\n",
      "|[-3.9311887350913...|       3.0|       3.0|\n",
      "|[4.93261867119043...|       4.0|       4.0|\n",
      "|[8.66099912148604...|       1.0|       1.0|\n",
      "|[10.6187336459507...|       1.0|       1.0|\n",
      "|[12.4764321663229...|       3.0|       3.0|\n",
      "|[16.3147134254692...|       3.0|       2.0|\n",
      "|[21.1733697878802...|       2.0|       2.0|\n",
      "|[27.9307098171451...|       4.0|       4.0|\n",
      "|[29.5026653660435...|       1.0|       1.0|\n",
      "|[30.5157508836321...|       4.0|       2.0|\n",
      "|[33.1589097915249...|       1.0|       1.0|\n",
      "|[36.3775090462241...|       1.0|       1.0|\n",
      "|[38.7207549754442...|       1.0|       1.0|\n",
      "|[41.4118672978134...|       4.0|       2.0|\n",
      "|[44.4927384799594...|       3.0|       3.0|\n",
      "|[48.2790218950527...|       2.0|       2.0|\n",
      "|[49.6646105387395...|       3.0|       3.0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2846:====================================================> (34 + 1) / 35]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outcome=modelClassify(test,CORF_classifiers)\n",
    "outcome.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7802633260897354"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=outcome.withColumn(\"outCome\",F.when(F.col(\"ClassIndex\")==F.col(\"prediction\"), 1).otherwise(0))\n",
    "accuracy=a.groupBy('outCome').sum('outCome').collect()[0][\"sum(outCome)\"]/a.count()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark accuracy: 0.780263\n",
      "pyspark f1-score: 0.779555\n",
      "pyspark precision: 0.783554\n",
      "pyspark recall: 0.780263\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"f1\")\n",
    "evaluator_pre = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"ClassIndex\", metricName=\"weightedRecall\")\n",
    "\n",
    "print('pyspark accuracy: %.6f' %evaluator_acc.evaluate(a))\n",
    "print('pyspark f1-score: %.6f' %evaluator_f1.evaluate(a))\n",
    "print('pyspark precision: %.6f' %evaluator_pre.evaluate(a))\n",
    "print('pyspark recall: %.6f' %evaluator_recall.evaluate(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitnami-python3",
   "language": "python",
   "name": "bitnami-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
