{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/spark-data/CW', '/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip', '/opt/bitnami/spark/python', '/opt/bitnami/spark/python/build', '/opt/bitnami/python/lib/python38.zip', '/opt/bitnami/python/lib/python3.8', '/opt/bitnami/python/lib/python3.8/lib-dynload', '', '/opt/bitnami/python/lib/python3.8/site-packages', '/opt/bitnami/python/lib/python3.8/site-packages/pip-21.3.1-py3.8.egg']\n",
      "/opt/bitnami/python/bin/python3\n",
      "3.8.13 (default, Mar 25 2022, 21:22:37) \n",
      "[GCC 8.3.0]\n",
      "sys.version_info(major=3, minor=8, micro=13, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "\n",
    "# import os\n",
    "# memory = '3g'\n",
    "# pyspark_submit_args = ' --driver-memory ' + memory + \" --executor-memory \" + \"3g\" +  ' pyspark-shell' \n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 13:39:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/30 13:39:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'master'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.app.id', 'local-1651325997673'),\n",
       " ('spark.executor.memory', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'CW'),\n",
       " ('spark.app.startTime', '1651325996759'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '32991'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "from pyspark.pandas.config import set_option, reset_option\n",
    "\n",
    "conf =SparkConf().setMaster(\"local[*]\").setAppName(\"CW\") \n",
    "SparkContext.setSystemProperty('spark.executor.memory', '1g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '1g')\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Postures.csv\"\n",
    "df = spark.read.option(\"header\",True).option(\"nullValue\",\"?\").csv(path)\n",
    "# df.show()\n",
    "# df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Class: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- X0: string (nullable = true)\n",
      " |-- Y0: string (nullable = true)\n",
      " |-- Z0: string (nullable = true)\n",
      " |-- X1: string (nullable = true)\n",
      " |-- Y1: string (nullable = true)\n",
      " |-- Z1: string (nullable = true)\n",
      " |-- X2: string (nullable = true)\n",
      " |-- Y2: string (nullable = true)\n",
      " |-- Z2: string (nullable = true)\n",
      " |-- X3: string (nullable = true)\n",
      " |-- Y3: string (nullable = true)\n",
      " |-- Z3: string (nullable = true)\n",
      " |-- X4: string (nullable = true)\n",
      " |-- Y4: string (nullable = true)\n",
      " |-- Z4: string (nullable = true)\n",
      " |-- X5: string (nullable = true)\n",
      " |-- Y5: string (nullable = true)\n",
      " |-- Z5: string (nullable = true)\n",
      " |-- X6: string (nullable = true)\n",
      " |-- Y6: string (nullable = true)\n",
      " |-- Z6: string (nullable = true)\n",
      " |-- X7: string (nullable = true)\n",
      " |-- Y7: string (nullable = true)\n",
      " |-- Z7: string (nullable = true)\n",
      " |-- X8: string (nullable = true)\n",
      " |-- Y8: string (nullable = true)\n",
      " |-- Z8: string (nullable = true)\n",
      " |-- X9: string (nullable = true)\n",
      " |-- Y9: string (nullable = true)\n",
      " |-- Z9: string (nullable = true)\n",
      " |-- X10: string (nullable = true)\n",
      " |-- Y10: string (nullable = true)\n",
      " |-- Z10: string (nullable = true)\n",
      " |-- X11: string (nullable = true)\n",
      " |-- Y11: string (nullable = true)\n",
      " |-- Z11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "# df.dtypes\n",
    "# the filling type must be consistent with the original type. Thus, filling string here.\n",
    "df=df.na.fill('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.where(df['Class']!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Class: string (nullable = false)\n",
      " |-- X0: string (nullable = false)\n",
      " |-- Y0: string (nullable = false)\n",
      " |-- Z0: string (nullable = false)\n",
      " |-- X1: string (nullable = false)\n",
      " |-- Y1: string (nullable = false)\n",
      " |-- Z1: string (nullable = false)\n",
      " |-- X2: string (nullable = false)\n",
      " |-- Y2: string (nullable = false)\n",
      " |-- Z2: string (nullable = false)\n",
      " |-- X3: string (nullable = false)\n",
      " |-- Y3: string (nullable = false)\n",
      " |-- Z3: string (nullable = false)\n",
      " |-- X4: string (nullable = false)\n",
      " |-- Y4: string (nullable = false)\n",
      " |-- Z4: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('Class','X0','Y0','Z0','X1','Y1','Z1','X2','Y2','Z2','X3','Y3','Z3','X4','Y4','Z4')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert String to Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:======>                                                    (1 + 8) / 9]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class    float64\n",
       "X0       float64\n",
       "Y0       float64\n",
       "Z0       float64\n",
       "X1       float64\n",
       "Y1       float64\n",
       "Z1       float64\n",
       "X2       float64\n",
       "Y2       float64\n",
       "Z2       float64\n",
       "X3       float64\n",
       "Y3       float64\n",
       "Z3       float64\n",
       "X4       float64\n",
       "Y4       float64\n",
       "Z4       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.toPandas()\n",
    "df2 = df2.astype('float')\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Filling of Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanList = []\n",
    "for i in range(1,6):\n",
    "    meanList.append(df2[(df2[\"Class\"] == i) & (df2[\"X3\"] != 0)][\"X3\"].mean())\n",
    "def fill_val1(col):\n",
    "    Class = col[0]\n",
    "    axis = col[1]\n",
    "    if (axis == 0):\n",
    "        \n",
    "        if Class == 1:\n",
    "            return meanList[0]\n",
    "        \n",
    "        elif Class == 2:\n",
    "            return meanList[1]\n",
    "        elif Class == 3:\n",
    "            return meanList[2]\n",
    "        elif Class == 4:\n",
    "            return meanList[3]\n",
    "        elif Class == 5:\n",
    "            return meanList[4]\n",
    "    else:\n",
    "        return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanList2 = []\n",
    "for i in range(1,6):\n",
    "    meanList2.append(df2[(df2[\"Class\"] == i) & (df2[\"Y3\"] != 0)][\"Y3\"].mean())\n",
    "def fill_val2(col):\n",
    "    Class = col[0]\n",
    "    axis = col[1]\n",
    "    if (axis == 0):\n",
    "        \n",
    "        if Class == 1:\n",
    "            return meanList2[0]\n",
    "        \n",
    "        elif Class == 2:\n",
    "            return meanList2[1]\n",
    "        elif Class == 3:\n",
    "            return meanList2[2]\n",
    "        elif Class == 4:\n",
    "            return meanList2[3]\n",
    "        elif Class == 5:\n",
    "            return meanList2[4]\n",
    "    else:\n",
    "        return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanList3 = []\n",
    "for i in range(1,6):\n",
    "    meanList3.append(df2[(df2[\"Class\"] == i) & (df2[\"Z3\"] != 0)][\"Z3\"].mean())\n",
    "def fill_val3(col):\n",
    "    Class = col[0]\n",
    "    axis = col[1]\n",
    "    if (axis == 0):\n",
    "\n",
    "        if Class == 1:\n",
    "            return meanList3[0]\n",
    "        \n",
    "        elif Class == 2:\n",
    "            return meanList3[1]\n",
    "        elif Class == 3:\n",
    "            return meanList3[2]\n",
    "        elif Class == 4:\n",
    "            return meanList3[3]\n",
    "        elif Class == 5:\n",
    "            return meanList3[4]\n",
    "    else:\n",
    "        return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmeanList(colName):\n",
    "    meanList = []\n",
    "    for i in range(1,6):\n",
    "        meanList.append(df2[(df2[\"Class\"] == i) & (df2[colName] != 0)][colName].mean())\n",
    "    return meanList\n",
    "def fill_val(meanList,col):\n",
    "    Class = col[0]\n",
    "    axis = col[1]\n",
    "    if (axis == 0):\n",
    "        if Class == 1:\n",
    "            return meanList[0]\n",
    "\n",
    "        elif Class == 2:\n",
    "            return meanList[1]\n",
    "        elif Class == 3:\n",
    "            return meanList[2]\n",
    "        elif Class == 4:\n",
    "            return meanList[3]\n",
    "        elif Class == 5:\n",
    "            return meanList[4]\n",
    "    else:\n",
    "        return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "df2['X3'] = df2[['Class', \"X3\"]].apply(fill_val1, axis = 1)\n",
    "df2['Y3'] = df2[['Class', \"Y3\"]].apply(fill_val2, axis = 1)\n",
    "df2['Z3'] = df2[['Class', \"Z3\"]].apply(fill_val3, axis = 1)\n",
    "\n",
    "meanfill=getmeanList('X4')\n",
    "func=functools.partial(fill_val,meanfill)\n",
    "df2['X4'] = df2[['Class', \"X4\"]].apply(func, axis = 1)\n",
    "\n",
    "meanfill=getmeanList('Y4')\n",
    "func=functools.partial(fill_val,meanfill)\n",
    "df2['Y4'] = df2[['Class', \"Y4\"]].apply(func, axis = 1)\n",
    "\n",
    "meanfill=getmeanList('Z4')\n",
    "func=functools.partial(fill_val,meanfill)\n",
    "df2['Z4'] = df2[['Class', \"Z4\"]].apply(func, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Class: double (nullable = true)\n",
      " |-- X0: double (nullable = true)\n",
      " |-- Y0: double (nullable = true)\n",
      " |-- Z0: double (nullable = true)\n",
      " |-- X1: double (nullable = true)\n",
      " |-- Y1: double (nullable = true)\n",
      " |-- Z1: double (nullable = true)\n",
      " |-- X2: double (nullable = true)\n",
      " |-- Y2: double (nullable = true)\n",
      " |-- Z2: double (nullable = true)\n",
      " |-- X3: double (nullable = true)\n",
      " |-- Y3: double (nullable = true)\n",
      " |-- Z3: double (nullable = true)\n",
      " |-- X4: double (nullable = true)\n",
      " |-- Y4: double (nullable = true)\n",
      " |-- Z4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(df2) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.write.csv(\"/opt/spark-data/CW/new_feature2.csv\")\n",
    "\n",
    "df.toPandas().to_csv('new_feature2.csv',index=False)\n",
    "df = df.select('Class','X0','Y0','Z0','X1','Y1','Z1','X2','Y2','Z2','X3','Y3','Z3')\n",
    "df.toPandas().to_csv('new_feature.csv',index=False)\n",
    "\n",
    "# df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up, uncomment this and run directly\n",
    "# path = \"./new_feature.csv\"\n",
    "# df = spark.read.option(\"header\",True).csv(path)\n",
    "# df = df.to_pandas_on_spark()\n",
    "# df = df.astype('float')\n",
    "# print(df.dtypes)\n",
    "# df = df.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 13:42:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([54.2639, 71.4668, -64.8077, 76.8956, 42.4625, -72.7805, 36.6212, 81.6806, -52.9193, 85.2323, 67.7492, -73.6841, 59.1886, 10.6789, -71.2978]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import allclose\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from  pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "assembler = VectorAssembler(inputCols=df.drop('Class').columns,outputCol=\"features\")\n",
    "df=assembler.transform(df)\n",
    "df.select('features').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/29 15:06:50 WARN TaskSetManager: Stage 6 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:50 WARN TaskSetManager: Stage 7 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:51 WARN TaskSetManager: Stage 9 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:52 WARN TaskSetManager: Stage 11 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:53 WARN TaskSetManager: Stage 13 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:53 WARN TaskSetManager: Stage 15 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:54 WARN TaskSetManager: Stage 17 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:55 WARN DAGScheduler: Broadcasting large task binary with size 1176.6 KiB\n",
      "22/04/29 15:06:55 WARN TaskSetManager: Stage 19 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/04/29 15:06:57 WARN TaskSetManager: Stage 21 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:06:59 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "22/04/29 15:06:59 WARN TaskSetManager: Stage 23 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1295.6 KiB\n",
      "22/04/29 15:07:02 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "22/04/29 15:07:02 WARN TaskSetManager: Stage 25 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:07:06 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/04/29 15:07:08 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n",
      "22/04/29 15:07:08 WARN TaskSetManager: Stage 27 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/29 15:07:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "# X_train = train.drop('ClassIndex')\n",
    "# y_train = train.select('ClassIndex')\n",
    "# X_test = test.drop('ClassIndex')\n",
    "# y_test = test.select('ClassIndex')\n",
    "# print(\"The number of rows in training set is: \",X_train.count(),\". The number of columns in training set is \",len(X_train.columns))\n",
    "# print(\"The number of rows in testing is: \",X_test.count(),\". The number of columns in testing set is \",len(X_test.columns))\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rtree = RandomForestClassifier(100)\n",
    "# rtree.fit(X_train, y_train)\n",
    "# prediction = rtree.predict(X_test)\n",
    "rf = RandomForestClassifier(numTrees=100,maxDepth=10,maxBins=32,labelCol = 'ClassIndex',featuresCol = 'features') # maxDepth=2, labelCol=\"indexed\", seed=42, leafCol=\"leafId\"\n",
    "rf_model = rf.fit(train)\n",
    "rf_predictions = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/29 15:07:31 WARN DAGScheduler: Broadcasting large task binary with size 12.7 MiB\n",
      "22/04/29 15:07:31 WARN TaskSetManager: Stage 29 contains a task of very large size (1033 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 29:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accu: 0.9130564050288821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:==============>                                          (3 + 9) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report,confusion_matrix\n",
    "# print(classification_report(y_test,prediction))\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Binary Classfication\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# binary_evaluator = BinaryClassificationEvaluator(labelCol = 'ClassIndex')\n",
    "# print('Random Forest:' , binary_evaluator.evaluate(rf_predictions))\n",
    "\n",
    "# Multiclass Classification\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'ClassIndex', metricName = 'accuracy')\n",
    "print(multi_evaluator.getMetricName())\n",
    "print('Random Forest Accu:', multi_evaluator.evaluate(rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/02 12:48:35 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
      "22/04/02 12:48:36 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
      "/opt/bitnami/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/04/02 12:48:39 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
      "22/04/02 12:48:39 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4486.   70.   88.   85.  135.]\n",
      " [  35. 4822.   18.   13.    8.]\n",
      " [ 156.    6. 4311.  252.   55.]\n",
      " [ 172.   16.  229. 3968.  161.]\n",
      " [ 560.   26.   61.  237. 3574.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# y_true = predictions_train.select(['ClassIndex']).collect()\n",
    "# y_pred = predictions_train.select(['prediction']).collect()\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print(confusion_matrix(y_true,y_pred))\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = rf_predictions.select(['prediction','ClassIndex']).withColumn('label', F.col('ClassIndex').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitnami-python3",
   "language": "python",
   "name": "bitnami-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
