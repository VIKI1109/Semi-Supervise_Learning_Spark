{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhEgFPcQgC-S"
      },
      "source": [
        "## Tri-training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3x8W0hiTgC-U"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext,SparkConf\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType,StringType,DoubleType\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
        "import sklearn\n",
        "import pandas as pd \n",
        " \n",
        "from pyspark.sql.functions import col\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The implementation of Tri-training algorithm\n",
        "This part of the code is the implementation of Tri-training algorithm. "
      ],
      "metadata": {
        "id": "ucKTpBe4N0uZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qs0lbQBsIubr"
      },
      "outputs": [],
      "source": [
        "class TriTraining:\n",
        "    def __init__(self, classifier):\n",
        "\n",
        "            \n",
        "              self.classifiers = [classifier[0],classifier[1],classifier[2]] \n",
        "              self.models=[[]]*3\n",
        "              \n",
        "\n",
        "    def fit(self, train):\n",
        "        #split to label and unlabel set\n",
        "        L_train, U_train = train.randomSplit([1-unlabel_ratio, unlabel_ratio], seed=rand_seed)\n",
        "\n",
        "        U_X = U_train.drop(\"ClassIndex\")\n",
        "        \n",
        "        L_X, L_Y ,L_Full= [[]] * 3, [[]] * 3 ,[[]] * 3  \n",
        " \n",
        "        L_train_x= L_train.select(\"features\")\n",
        "        L_train_y= L_train.select(\"ClassIndex\")\n",
        "\n",
        "        L_np=L_train.toPandas();\n",
        "\n",
        "        # boostrap\n",
        "        for i in range(3):\n",
        "            sample = sklearn.utils.resample(L_np)\n",
        "            L_Full[i]=spark.createDataFrame(sample) \n",
        "            L_Full[i].cache()\n",
        "            self.models[i]=self.classifiers[i].fit(L_Full[i])\n",
        "  \n",
        "                          \n",
        "        e_previous = [0.5] * 3 #default upper bound error\n",
        "        l_previous = [0] * 3\n",
        "        e = [0] * 3 \n",
        "        update = [False] * 3\n",
        "  \n",
        "        Li_X, Li_y = [[]] * 3, [[]] * 3  \n",
        "\n",
        "        improve = True\n",
        "        self.iter = 0\n",
        "\n",
        "        while improve:\n",
        "                  self.iter += 1  \n",
        "\n",
        "                  for i in range(3):\n",
        "                      \n",
        "\n",
        "                      j, k = np.delete(np.array([0, 1, 2]), i)\n",
        "                      update[i] = False\n",
        "                    \n",
        "                      # obtain the classification error rate of the hypothesis from the combination of the other two classifier\n",
        "                      e[i] = self.measure_error(L_train_x, L_train_y,L_train, j, k) \n",
        "                      \n",
        "                      \n",
        "                      if e[i] < e_previous[i]:\n",
        "\n",
        "                          Unlabel_j = self.models[j].transform(U_X)\n",
        "                          Unlabel_k = self.models[k].transform(U_X)\n",
        "                          # Join two DataFrames containing the prediction results of hj and hk and find the sample that hj and hk agree\n",
        "                          #on the labeling of the example by filter()\n",
        "                          Unlabel_j=Unlabel_j.join(Unlabel_k, (Unlabel_j[\"features\"] == Unlabel_k[\"features\"]),\"left\")\\\n",
        "                                             .withColumn(\"sub\", when(Unlabel_k.prediction == Unlabel_j.prediction, 0).otherwise(1))\\\n",
        "                                             .drop(Unlabel_k[\"features\"])\\\n",
        "                                             .drop(Unlabel_k[\"probability\"])\\\n",
        "                                             .drop(Unlabel_k[\"rawPrediction\"])\\\n",
        "                                             .drop(Unlabel_k[\"prediction\"])\n",
        "\n",
        "                          \n",
        "                          Unlabel_j_equal= Unlabel_j.filter(Unlabel_j.sub == 0)\n",
        "                          Li_X[i]=Unlabel_j_equal\n",
        "                          Unlabel_j_y= Unlabel_j_equal.select(\"prediction\")\n",
        "                          Li_y[i]=Unlabel_j_y\n",
        "                          \n",
        "                          \n",
        "                          \n",
        "                          if l_previous[i] == 0:  # no updated before\n",
        "                              l_previous[i] = int(e[i] / (e_previous[i] - e[i]) + 1)\n",
        "                          if l_previous[i] < Li_X[i].count():\n",
        "                              if e[i] * Li_X[i].count() < e_previous[i] * l_previous[i]:\n",
        "                                    update[i] = True\n",
        "                              elif l_previous[i] > e[i] / (e_previous[i] - e[i]):  #after subsampling the size of current Li_X will be still larger than l_previous[i]\n",
        "                                    x = int(e_previous[i] * l_previous[i] / e[i] - 1)\n",
        "                                    y = Li_y[i].count()    \n",
        "                                    Li_X[i]= Li_X[i].sample(False,x/y,42) #random select x/y sample in unlabel dataset, \n",
        "                                    Li_y[i] = Li_X[i].select(\"prediction\")\n",
        "                                    update[i] = True\n",
        "                          \n",
        "                          \n",
        "                  for i in range(3):\n",
        "                          if update[i]:\n",
        "                              Li_X[i]=Li_X[i].drop(\"probability\",\"rawPrediction\",\"sub\");\n",
        "                              Li_X[i]=Li_X[i].withColumnRenamed(\"prediction\",\"ClassIndex\")\n",
        "                              Li_X[i] = Li_X[i].select([\"features\",\"ClassIndex\"])\n",
        "                              L_Union=L_train.union(Li_X[i])\n",
        "                              L_Union.cache()  \n",
        "                              self.models[i]=self.classifiers[i].fit(L_Union) #retrain the model by the augumented data\n",
        "                              e_previous[i] = e[i]\n",
        "                              l_previous[i] = Li_y[i].count()\n",
        "                              L_Union.unpersist()\n",
        "\n",
        "                  Unlabel_j_equal.unpersist()\n",
        "                  Unlabel_j_y.unpersist()   \n",
        "                  Li_X[i].unpersist()\n",
        "                  Li_y[i].unpersist()    \n",
        "\n",
        "\n",
        "                  if update == [False] * 3:\n",
        "                          improve = False  # if no classifier was updated, no improvement and break the loop\n",
        "\n",
        "        return self.models;\n",
        "\n",
        "\n",
        "# The classification error rate of the hypothesis is approximated by dividing the number of labeled examples on which both model j and k make\n",
        "# incorrect classification by the number of labeled examples on which the classification made by j is the same as that made by k.\n",
        "    def measure_error(self, X, y, L_train, j, k):\n",
        "       \n",
        "        j_pred = self.models[j].transform(X)\n",
        "        k_pred = self.models[k].transform(X)\n",
        "\n",
        "        k_pred = k_pred.select(\"features\",\"prediction\")\n",
        "        j_pred = j_pred.select(\"features\",\"prediction\")\n",
        "        L= L_train.join(j_pred,(j_pred[\"features\"] == L_train[\"features\"]),'left')\\\n",
        "            .withColumn(\"j_right_prediction\", when(j_pred.prediction == L_train.ClassIndex, 1)\\\n",
        "            .otherwise(0))\\\n",
        "            .drop(j_pred.features)\n",
        "\n",
        "        L= L.join(k_pred,(k_pred[\"features\"] == L[\"features\"]),'left')\\\n",
        "            .withColumn(\"j_k_wrong_prediction\",when ((j_pred.prediction==k_pred.prediction)\\\n",
        "             & (L.j_right_prediction==0) , 1).otherwise(0))\\\n",
        "            .withColumn(\"j_k_same_prediction\",when (j_pred.prediction==k_pred.prediction, 1)\\\n",
        "            .otherwise(0))\n",
        "\n",
        "        j_index=L.filter(L.j_k_same_prediction== 1)\n",
        "        index = j_index.count()\n",
        "\n",
        "        j_wrong_index=L.filter(L.j_k_wrong_prediction== 1)\n",
        "        wrong_index = j_wrong_index.count()\n",
        "     \n",
        "        \n",
        "        L.unpersist()\n",
        "        j_pred.unpersist()\n",
        "        k_pred.unpersist()\n",
        "        \n",
        "\n",
        "        return wrong_index / index\n",
        "\n",
        "# Voting mechanism and ensemble learning: if there are more than one classifier with the same prediction result, the predicted value should be this result.\n",
        "    def predict(self, X):\n",
        "            \n",
        "            \n",
        "              pred_0 =  self.models[0].transform(X)\n",
        "              pred_1 =  self.models[1].transform(X)\n",
        "              pred_2 =  self.models[2].transform(X)\n",
        "              # subtract to obtain whether the two model have same prediction results\n",
        "              pred_1= pred_1.join(pred_2,(pred_1[\"features\"] == pred_2[\"features\"]),\"left\").withColumn(\"sub12\", pred_1.prediction -pred_2.prediction)\\\n",
        "                            .drop(pred_2[\"features\"])\\\n",
        "                            .drop(pred_2[\"probability\"])\\\n",
        "                            .drop(pred_2[\"rawPrediction\"])\\\n",
        "                            .drop(pred_2[\"prediction\"])\n",
        "              pred_1= pred_1.withColumnRenamed(\"prediction\",\"pred_prediction\")\n",
        "              \n",
        "              pred_0= pred_0.join(pred_1,(pred_0[\"features\"] == pred_1[\"features\"]),\"left\")\\\n",
        "                            .drop(pred_1[\"features\"])\\\n",
        "                            .drop(pred_1[\"probability\"])\\\n",
        "                            .drop(pred_1[\"rawPrediction\"])\\\n",
        "              # If the other two model agree on the results, then set this result as the final prediction results\n",
        "              pred_0= pred_0.withColumn(\"prediction\", when(pred_0.sub12==0,pred_0.pred_prediction).otherwise(pred_0.prediction))\n",
        "\n",
        "              return pred_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9XmCAC9gC-a"
      },
      "source": [
        "# Initialization\n",
        " This part of the code initializes spark session, dataframe and split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d5o0mlgNgC-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97265587-d98a-411f-b09b-0153ada6498c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[features: vector, ClassIndex: double]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from numpy import allclose\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from  pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Posture') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "path = \"new_feature.csv\"\n",
        "\n",
        "schema = StructType()\\\n",
        "      .add(\"Class\",DoubleType(),True) \\\n",
        "      .add(\"X0\",DoubleType(),True) \\\n",
        "      .add(\"Y0\",DoubleType(),True) \\\n",
        "      .add(\"Z0\",DoubleType(),True) \\\n",
        "      .add(\"X1\",DoubleType(),True) \\\n",
        "      .add(\"Y1\",DoubleType(),True) \\\n",
        "      .add(\"Z1\",DoubleType(),True) \\\n",
        "      .add(\"X2\",DoubleType(),True) \\\n",
        "      .add(\"Y2\",DoubleType(),True) \\\n",
        "      .add(\"Z2\",DoubleType(),True) \\\n",
        "      .add(\"X3\",DoubleType(),True) \\\n",
        "      .add(\"Y3\",DoubleType(),True) \\\n",
        "      .add(\"Z3\",DoubleType(),True) \\\n",
        "\n",
        "df = spark.read.option(\"header\",True).schema(schema).csv(path)\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=df.drop('Class').columns,outputCol=\"features\")\n",
        "indexer = StringIndexer(inputCol = 'Class', outputCol = 'ClassIndex')\n",
        "df=assembler.transform(df)\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "df=df.select('features', 'ClassIndex')\n",
        "num=df.rdd.getNumPartitions()\n",
        "df = df.repartition(num*3) \n",
        "\n",
        "unlabel_ratio=0.95\n",
        "label_ratio=0.05\n",
        "test_ratio=0.10\n",
        "rand_seed=np.random.randint(99999)\n",
        "\n",
        "\n",
        "train, test = df.randomSplit([1-test_ratio, test_ratio], seed=rand_seed)\n",
        "train.cache()\n",
        "test.cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        "This part of the code initialize and train our model"
      ],
      "metadata": {
        "id": "_NuZKaEsTn1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MMNcTiCQGcNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e392f91-5e79-467a-d523-9eeb7bbfc187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|            features|ClassIndex|\n",
            "+--------------------+----------+\n",
            "|[-51.253731022324...|       1.0|\n",
            "|[-28.911238153551...|       2.0|\n",
            "|[-28.062389062568...|       2.0|\n",
            "|[-26.715981589456...|       2.0|\n",
            "|[-24.423199379777...|       2.0|\n",
            "|[-21.962989974915...|       0.0|\n",
            "|[-21.379566046016...|       1.0|\n",
            "|[-21.372414890111...|       1.0|\n",
            "|[-21.334013756175...|       1.0|\n",
            "|[-20.266315168748...|       2.0|\n",
            "|[-19.016337895788...|       2.0|\n",
            "|[-18.859860263954...|       4.0|\n",
            "|[-18.664830265849...|       1.0|\n",
            "|[-17.262166457941...|       2.0|\n",
            "|[-16.928109152465...|       2.0|\n",
            "|[-16.650486451779...|       2.0|\n",
            "|[-16.580251840588...|       1.0|\n",
            "|[-16.468579851620...|       3.0|\n",
            "|[-14.909475968550...|       0.0|\n",
            "|[-14.667052706879...|       0.0|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[RandomForestClassificationModel: uid=RandomForestClassifier_e28523cc951e, numTrees=4, numClasses=5, numFeatures=12,\n",
              " RandomForestClassificationModel: uid=RandomForestClassifier_a350297f4c6e, numTrees=4, numClasses=5, numFeatures=12,\n",
              " RandomForestClassificationModel: uid=RandomForestClassifier_b54ce1558543, numTrees=4, numClasses=5, numFeatures=12]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "n=4\n",
        "m=12\n",
        "\n",
        "\n",
        "triTraining=TriTraining([RandomForestClassifier(numTrees=n,maxDepth=m,labelCol='ClassIndex',featuresCol='features',bootstrap=False),\n",
        "                         RandomForestClassifier(numTrees=n,maxDepth=m,labelCol='ClassIndex',featuresCol='features',bootstrap=False),\n",
        "                         RandomForestClassifier(numTrees=n,maxDepth=m,labelCol='ClassIndex',featuresCol='features',bootstrap=False)])\n",
        "\n",
        "triTraining.fit(train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1o7OlmgC-b"
      },
      "source": [
        "# Test\n",
        "This part of the code tests our model on the test data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MZW3o54onHqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3faed7-8212-4b18-8384-ddcf664be1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---------------+\n",
            "|prediction|sub12|pred_prediction|\n",
            "+----------+-----+---------------+\n",
            "|       2.0|  0.0|            2.0|\n",
            "|       1.0| -1.0|            1.0|\n",
            "|       2.0|  1.0|            2.0|\n",
            "|       0.0|  0.0|            0.0|\n",
            "|       3.0|  0.0|            3.0|\n",
            "|       1.0|  0.0|            1.0|\n",
            "|       1.0|  0.0|            1.0|\n",
            "|       2.0|  0.0|            2.0|\n",
            "|       2.0|  0.0|            2.0|\n",
            "|       1.0|  0.0|            1.0|\n",
            "|       0.0| -4.0|            0.0|\n",
            "|       3.0|  0.0|            3.0|\n",
            "|       4.0|  0.0|            4.0|\n",
            "|       2.0|  0.0|            2.0|\n",
            "|       3.0|  0.0|            3.0|\n",
            "|       2.0|  0.0|            2.0|\n",
            "|       1.0|  0.0|            1.0|\n",
            "|       4.0|  0.0|            4.0|\n",
            "|       0.0| -4.0|            0.0|\n",
            "|       0.0|  0.0|            0.0|\n",
            "+----------+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "testdata=test.select('features')\n",
        "testlabel=test.select('ClassIndex')\n",
        "predict=triTraining.predict(testdata)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "This part of the code evaluates the performance of the model on the test data set"
      ],
      "metadata": {
        "id": "ll6bA7a2Vfy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "test1=test.join(predict,(test[\"features\"] == predict[\"features\"]),\"left\").withColumn(\"outcome\",when(test.ClassIndex == predict.prediction, 1).otherwise(0))\n",
        "\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"accuracy\")\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"f1\")\n",
        "evaluator_pre = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ClassIndex\",metricName=\"weightedPrecision\")\n",
        "evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"ClassIndex\", metricName=\"weightedRecall\")\n",
        "\n",
        "print('pyspark accuracy: %.6f' %evaluator_acc.evaluate(test1))\n",
        "print('pyspark f1-score: %.6f' %evaluator_f1.evaluate(test1))\n",
        "print('pyspark precision: %.6f' %evaluator_pre.evaluate(test1))\n",
        "print('pyspark recall: %.6f' %evaluator_recall.evaluate(test1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob8XmK1gVe6M",
        "outputId": "8bf71351-56a4-4bfd-c60a-0aafc4187c47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyspark accuracy: 0.811398\n",
            "pyspark f1-score: 0.810928\n",
            "pyspark precision: 0.813446\n",
            "pyspark recall: 0.811398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop\n",
        "This part of the code stops the spark session"
      ],
      "metadata": {
        "id": "SKUQMGqcXHTl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PUhQjk70LaaI"
      },
      "outputs": [],
      "source": [
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sRdF6S28FFyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ma3XWZNgC-c"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaZt7b9-gC-c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tri-training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bitnami-python3",
      "language": "python",
      "name": "bitnami-python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}